基础作业

配置 LMDeploy 运行环境
![c37f4af30202363f7abe63d3b45d1120](https://github.com/lyhcreate/InternLM_demo/assets/93357834/198a0e5d-f755-4ca3-8797-c5650e4de151)


以命令行方式与 InternLM2-Chat-1.8B 模型对话
```
conda activate lmdeploy
lmdeploy chat /root/internlm2-chat-1_8b
```
让InternLM2-Chat-1.8B 给我讲了一个笑话
![716d0eb3f17c5d5ee7727eefc2654205](https://github.com/lyhcreate/InternLM_demo/assets/93357834/a1f07586-df03-4ad2-9986-f40234b898ba)


进阶作业
设置KV Cache最大占用比例为0.4，开启W4A16量化，以命令行方式与模型对话。
开启W4A16量化
```
lmdeploy lite auto_awq \
   /root/internlm2-chat-1_8b \
  --calib-dataset 'ptb' \
  --calib-samples 128 \
  --calib-seqlen 1024 \
  --w-bits 4 \
  --w-group-size 128 \
  --work-dir /root/internlm2-chat-1_8b-4bit
```
设置KV Cache最大占用比例为0.4
```
lmdeploy chat /root/internlm2-chat-1_8b-4bit --model-format awq --cache-max-entry-count 0.4
```

显存占用11492mb
![image](https://github.com/lyhcreate/InternLM_demo/assets/93357834/45cfc267-307a-4ef9-8334-982474729a9f)

```
conda activate lmdeploy
```
以API Server方式启动 lmdeploy，开启 W4A16量化，调整KV Cache的占用比例为0.4，分别使用命令行客户端与Gradio网页客户端与模型对话。（优秀学员必做）
运行服务器命令
```
lmdeploy serve api_server \
    /root/internlm2-chat-1_8b \
    --model-format hf \
    --quant-policy 0 \
    --server-name 0.0.0.0 \
    --server-port 23333 \
    --tp 1
```
命令行运行
```
lmdeploy serve api_client http://localhost:23333
```
![image](https://github.com/lyhcreate/InternLM_demo/assets/93357834/3d327b81-cc2c-4f69-a5a2-ca4ccd16ff2a)

网页端运行
```
lmdeploy serve gradio http://localhost:23333 \
    --server-name 0.0.0.0 \
    --server-port 6006
```
![image](https://github.com/lyhcreate/InternLM_demo/assets/93357834/c501f0a1-e87e-4405-ab35-68225f41d2eb)


使用W4A16量化，调整KV Cache的占用比例为0.4，使用Python代码集成的方式运行internlm2-chat-1.8b模型。（优秀学员必做）
使用 LMDeploy 运行视觉多模态大模型 llava gradio demo。（优秀学员必做）
将 LMDeploy Web Demo 部署到 OpenXLab 。

